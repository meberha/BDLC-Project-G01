{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8142c433-a560-4917-8f37-48f8e15790f1",
   "metadata": {},
   "source": [
    "# Dataflow\n",
    "\n",
    "## Wie werden die Daten End-to-End durch das System laufen?\n",
    "\n",
    "Die Daten werden als Raw Files direkt über die Kaggle-API auf die Master Node geladen, entpackt und dann unverändert ins HDFS geladen. Aus den Raw Files im HDFS werden dann zwei Parquet Files erstellt und die Originale werden gelöscht. Die Parquet Files werden dann in der Datenbank in Tables geschrieben.\n",
    "\n",
    "## Wie werden die Rohdaten gespeichert?\n",
    "\n",
    "Die Rohdaten werden nur kurz auf dem Master und später im HDFS zwischengespeichert. Sobald die Rohdaten im platzsparenderem Parquet Format gespeichert sind, werden sie gelöscht.\n",
    "\n",
    "## Sind die Daten partitioniert? Wenn ja, weshalb und wie?\n",
    "\n",
    "Die Parquet Files sind mit dem Partitionsfaktor 55 gespeichert. Dies entspricht einer generellen Empfehlung, den Partitionsfaktor so zu wählen, dass er 4 x der verfügbaren Cores entspricht. Die Partitionierung wurde beim Erstellen der Parquet Files gemacht.\n",
    "\n",
    "## In welchem Format sind die prozessierten Daten gespeichert? Und wieso?\n",
    "\n",
    "Die prozessierten Daten sind als Parquet Files gespeichert. Diese Files sind dank der effizienten Kompression sehr platzsparend und können schnell und effizient gelesen werden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
