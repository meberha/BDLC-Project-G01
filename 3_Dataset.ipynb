{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9abfcf-1c9d-4cbd-ba19-b5fcdc52f7e3",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49138e8-dc17-48e2-8e6a-830e0185dfc0",
   "metadata": {},
   "source": [
    "### Wieso haben Sie sich für diesen Datensatz entschieden?\n",
    "\n",
    "Wir haben im Internet (kaggle.com) nach Datensätzen recherchiert, welche einem «Big-Data» Ansatz entsprechen. Wir mussten aber feststellen, dass reine Text Datensätze, welche für unseren Cluster passen würden, schwer zu finden sind. Zusätzlich wollten wir ein Datensatz, der ein Thema beinhaltet, welches uns allen gängig ist. \n",
    "\n",
    "Daher haben wir uns für das «Airline on-time Performance Data» Dataset entschieden, da dieses mit einer Grösse von ca. 12 GB perfekt zu unser Cluster Grösse passt und wir die Begriffe im Datensat korrekt verstanden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6e2fc-b721-4f0d-b7df-63d8dddcfe25",
   "metadata": {},
   "source": [
    "### Wie haben Sie den Datensatz heruntergeladen? (api, save as, csv, xml, json, ...)\n",
    "\n",
    "Wir haben die Daten von kaggle.com mitels eines API heruntergeladen. Die Daten werden als .csv Files auf der Master-VM zwischengespeichert. Anschliessend wurden die CSV-Files auf HDFS gespeichert und dort mittels SPARK in einzelne Parquet Files umformatiert. Schlussendlich wurden die alten CSV-Files gelöscht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c25cba7-5bd6-4c16-b940-b7a89c145d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a996fc8-5374-4ef5-bd9a-573e6bed943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133a870-2c3d-46d0-94d4-897d1d33970c",
   "metadata": {},
   "source": [
    "Herunterladen und entpacken der Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1f61fa-6de6-474c-8019-d8c7f98ba32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.dataset_download_file('bulter22/airline-data', 'carriers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd248196-9751-4765-8292-e72e1567783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.dataset_download_file('bulter22/airline-data', 'airline.csv.shuffle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c184455-24d0-4856-9f1c-137499d0a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = ZipFile('airline.csv.shuffle.zip')\n",
    "zf.extractall()\n",
    "zf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66bb10-e168-4a1b-bb36-2dcac5502aba",
   "metadata": {},
   "source": [
    "### Wie haben Sie die Daten ins HDFS geladen? Musste die Blocksize von HDFS angepasst werden?\n",
    "\n",
    "Aus den .csv Files erstellten wir Parquet-Files, welcher ebenfall in HDFS verteilt werden. Da die Default-HDFS-Blocksize bereits 128MB beträgt und auch Parquet diese Blockgrösse standardmässig aufweist, musste nichts weiteres veranlasst werden. Somit kann ein ganzer Block vom Parquet File von einem Block des HDFS gelesen werden.\n",
    "\n",
    "Für die Partitionierung der Parquet-Files haben wir den Faktor 55 gewählt. Dieser Faktor wurde gewählt, da wir für SPARK gemäss Topologie insgesamt 55 CPU's für die Berechnungen zur Verfügung haben. Dadurch werden die SPARK-Jobs auf alle CPU's verteilt, was eine optimale Parallelisierung ermöglicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85e145-9b5a-48df-b562-4a8d6b1083e7",
   "metadata": {},
   "source": [
    "HDFS vorbereiten und Files kopieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07a6ccd-37eb-44d3-8f3f-13ca95687be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - cluster supergroup          0 2022-05-05 12:04 /user\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f50839-0e12-4ee2-93d4-09f33752a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /airline-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d289bf1-9686-4285-b495-b4f6f77c69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put ~/BDLC-Project-G01/airline.csv.shuffle /airline-data\n",
    "!hdfs dfs -put ~/BDLC-Project-G01/carriers.csv /airline-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3149125f-afbe-490f-a3c2-0742173cacfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   2 cluster supergroup 12029207752 2022-05-29 13:38 /airline-data/airline.csv.shuffle\n",
      "-rw-r--r--   2 cluster supergroup       43758 2022-05-29 13:38 /airline-data/carriers.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /airline-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288216ad-1f85-4bb2-addc-7570d82c766a",
   "metadata": {},
   "source": [
    "Konvertieren der Files zu Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbb179bd-9561-491b-992f-7520bbe40166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e43118-a5c4-48f3-9d41-bb6cb2cac8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-05-29 13:38:56,652 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('ConvertToParquet').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e8f0a49-2830-4c25-a343-e1d83b6a9286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.option('encoding', 'us-ascii').csv('/airline-data/carriers.csv', header=True).repartition(55).write.parquet('/airline-data/carriers.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3ab4ec-4cbb-4bde-9c40-fcae483d5a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 13:39:59,790 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "2022-05-29 13:40:49,178 WARN scheduler.TaskSetManager: Lost task 68.0 in stage 6.0 (TID 216) (10.177.124.99 executor 9): java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:549)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:283)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2022-05-29 13:41:08,951 WARN scheduler.TaskSetManager: Lost task 88.0 in stage 6.0 (TID 236) (10.177.124.99 executor 9): java.io.IOException: No space left on device\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n",
      "\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n",
      "\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:549)\n",
      "\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n",
      "\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:283)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.option('inferSchema', True).option('encoding', 'iso-8859-1').csv('/airline-data/airline.csv.shuffle', header=True).repartition(55).write.parquet('/airline-data/airline.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68e971-7f75-456c-bb40-4a2d3e9c391f",
   "metadata": {},
   "source": [
    "HDFS airline-data überprüfen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a65ec794-4bae-4333-967b-8680223797b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   2 cluster supergroup 12029207752 2022-05-29 13:38 /airline-data/airline.csv.shuffle\n",
      "drwxr-xr-x   - cluster supergroup           0 2022-05-29 13:42 /airline-data/airline.parquet\n",
      "-rw-r--r--   2 cluster supergroup       43758 2022-05-29 13:38 /airline-data/carriers.csv\n",
      "drwxr-xr-x   - cluster supergroup           0 2022-05-29 13:39 /airline-data/carriers.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /airline-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7127e4-8e5e-455e-b94c-778a413fe476",
   "metadata": {},
   "source": [
    "Originale löschen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6cc9ec-48d9-4069-96ca-b622ed6ab308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /airline-data/carriers.csv\n",
      "Deleted /airline-data/airline.csv.shuffle\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /airline-data/carriers.csv\n",
    "!hdfs dfs -rm /airline-data/airline.csv.shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11b9d05b-10dc-4861-aa19-719eae630915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - cluster supergroup          0 2022-05-29 13:42 /airline-data/airline.parquet\n",
      "drwxr-xr-x   - cluster supergroup          0 2022-05-29 13:39 /airline-data/carriers.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /airline-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b679078f-fc53-4d79-8080-66aef5ec4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f1e2d-c6fc-4eb1-a84c-4fe49c397195",
   "metadata": {},
   "source": [
    "### Ist Ihr Projekt ein Big-Data-Problem?\n",
    "\n",
    "Da erst ab einer Grösse im Terabyte Bereich von Big-Data gesprochen wird und die airline-Daten mit wenigen Ausnahmen einheitlich sind, kann man hier nicht von einem Big-Data-Problem sprechen. Um ein Big-Data-Problem auszuwerten ist unser Cluster im EnterpriseLab leider zu klein, es ist aber trotzdem eine spannende Übung, da das File doch rund 123 Millionen Datensätze enthält. Mit dieser Anzahl Datensätzen und der vorhandenen Maschinen-Kapazität kann doch eine aussagekräftige und performante Analyse gemacht werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e320035-e39f-4a13-8bd6-9e980bab0913",
   "metadata": {},
   "source": [
    "### Erklären Sie Ihre Daten. Was bedeuten die Felder, welches Schema sollten die Daten haben, gibt es primary keys und foreign keys?\n",
    "\n",
    "Die Daten des «Airline on-time Performance Data» Dataset beinhalten die Aufzeichnungen der Inlandflüge in den USA von 1987 - 2008. Untenstehende Tabellen erklärt die Felder und das Schema. Die zwei Tabellen \"Airlinedata\" und \"Carrier\" werden mit dem Primarschlüssel \"Code\" (bei Carrier)  und dem Fremdschlüssel \"UniqueCarrier\" (bei Airlinedata) verbunden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd250f-c010-4ce7-af6f-4e1a013674d8",
   "metadata": {},
   "source": [
    "#### Airlinedata\n",
    "| Name | Beschreibung| Datentyp |\n",
    "| ------- | -------- | -------- |\n",
    "| Year | Jahr | int |\n",
    "| Month | Monat | int |\n",
    "| Day of Month | Tag des Monat (1-31) | int |\n",
    "| DayOfWeek | Wochentag (1 = Monday - 7 = Sunday) | int |\n",
    "| DepTime | Reale Abflugszeit (lokal, hhmm) | int |\n",
    "| CRSDepTime | Geplante Abflugszeit (lokal, hhmm) | int |\n",
    "| ArrTime | Reale Ankunftszeit (lokal, hhmm) | int |\n",
    "| CRSArrTime | Geplante Ankunftszeit (lokal, hhmm) | int |\n",
    "| UniqueCarrier | Fluggesellschafts Code | string |\n",
    "| FlightNum | Flugnummer | string |\n",
    "| TailNum | Flugzeugnummer | string |\n",
    "| ActualElapsedTime | Tatsächliche Zeitdauer (in Minuten) |  int |\n",
    "| CRSElapsedTime | Geplante Zeitdauer (in Minuten) | int |\n",
    "| AirTime | Flugdauer (in Minuten) | int |\n",
    "| ArrDelay | Ankunft Verspätung (in Minuten) | int |\n",
    "| DepDelay | Abflug Verspätung (in Minuten) | int |\n",
    "| Origin | Startort IATA Flughafen Code | string |\n",
    "| Dest | Zielort IATA Flughafen Code | string |\n",
    "| Distance | Distanz (in Meilen) |  int |\n",
    "| TaxiIn | Zeit in Bewegung am Boden bei Abflug (in Minuten) | int |\n",
    "| TaxiOut | Zeit in Bewegung am Boden bei Ankunft (in Minuten) | int |\n",
    "| Cancelled | Wurde der Flug gestrichen? | string |\n",
    "| CancellationCode | Ursache der streichnung des Flugs (A = carrier, B = weather, C = NAS, D = security) | string |\n",
    "| Diverted | Flug wurde Umgeleitet (1 = ja, 0 = nein) | int |\n",
    "| CarrierDelay | Verspätung wegen des Gepäcks (in Minuten) | int | \n",
    "| WeatherDelay | Verspätung wegen des Wetters (in Minuten) | int |\n",
    "| NASDelay | \"National Aviation System\" Verspätung (in Minuten) | int |\n",
    "| SecurityDelay | Verspätung wegen der Sicherheit (in Minuten) | int |\n",
    "| LateAircraftDelay | Verspätung wegen dem Flugzeug (in Minuten) | int |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d531e390-9ae7-4398-b596-7962c7056ce9",
   "metadata": {},
   "source": [
    "#### Carrier\n",
    "| Name | Beschreibung| Datentyp |\n",
    "| ------- | -------- | -------- |\n",
    "| Code | Fluggesellschafts Code | string |\n",
    "| Description | Name der Airline | string |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
