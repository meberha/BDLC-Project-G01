{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f7c324-bbc6-4085-b058-84eac85494e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Virtuelle Maschinen\n",
    "### Gruppe 1\n",
    "Unsere Gruppe umfasst folgende VMs:\n",
    "\n",
    "    bdlc-05 = Matthias Eberhard\n",
    "    bdlc-11 = Philippe Michel\n",
    "    bdlc-15 = Corinne Schwarzentruber\n",
    "    \n",
    "Wir haben uns beim Aufsetzen des Clusters an das Beispiel gemäss dem Unterricht orientiert. Wir umfassen ebenfalls drei virtuelle Maschienen, womit unser Datensatz von ca. 12 GB gut bearbeitet werden kann.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f65747f-72f7-432b-9e13-247ee3479bfd",
   "metadata": {},
   "source": [
    "## Topologie\n",
    "Nachfolgend ist die Übersicht unserer Topologie visuell dargestellt. \n",
    "\n",
    "Es ist ersichtlich, dass von den 47 GB RAM insgesamt 44 GB den Services zugeteilt wird. Somit verfügt jeder Node über 3 GB (oder Master 4 GB) restliche RAMs, für andere Systeme (z.Bsp. Java, Jupyter).\n",
    "\n",
    "Die 20 Prozessoren wurden bei bdlc-11 und bdlc-15 vollständig dem SPARK zugeteilt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed3cdd-1463-4e13-a029-f612f8cdb87a",
   "metadata": {},
   "source": [
    "<img src=\"V10_TopologieCluster.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8fc64-6a46-475b-a113-fa8b7c9f3952",
   "metadata": {},
   "source": [
    "### HDFS und Hadoop\n",
    "http://bdlc-05.el.eee.intern:9870/\n",
    "\n",
    "Um den Datensatz über den Cluster verteilt zu speichern und dadurch auch Redundanzen zu schaffen (für Ausfallsicherung, sollte eine VM aussteigen) installieren wir HADOOP HDFS. Dies ermöglicht auch, dass nur die Berechnungen auf dem Datensatz über das Netz laufen. Und selbstverständlich, dass wir insgesamt nun 55 Prozessoren und 121 GB RAM ansteuern können. Dies erlaubt Analysen auf grösseren Datensätzen mit einer befriedigenden Responce-Time.\n",
    "\n",
    "HDFS hat eine Master/Slave Architektur, wobei unsere VM-05 der Master ist (NameNode) und die weiteren 2 VMs sind die Slaves (DataNodes). Um die Ausfallsicherheit zu gewährleisten, muss auch der NameNode duppliziert werden. Denn nur dieser weiss, welche Datenpackete auf welcher VM gespeichert ist (incl. Metadaten). Somit initialisieren wir einen SecondNameNode auf der VM-11.\n",
    "\n",
    "Wir definieren, dass wir uns auf einen Replikationsfaktor von 2 beschränken (der Default wäre 3). Da durch die Replikation die Datenmenge multipliziert wird, macht es keinen Sinn, diesen zu hoch anzusetzen. Um das Kriterium der Ausfallsicherheit zu erfüllen, muss mindestens ein Faktor von 2 gewählt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0b2d54-50ff-4586-8b10-6e449a56f42b",
   "metadata": {},
   "source": [
    "## HIVE\n",
    "HIVE wäre ein Dienst, um verteilte SQL-Queries zu starten. Da wir uns für SPARK entschieden haben, benötigen wir HIVE nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae6976-6077-441f-9143-220227fedd75",
   "metadata": {},
   "source": [
    "## YARN\n",
    "\n",
    "YARN ist ebenfalls ein Dienst von Hadoop, um verteilt zu berechnen. Da wir uns für SPARK entschieden haben, haben wir YARN nicht im Einsatz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59e95a-0b30-42be-9e90-1f48329f895e",
   "metadata": {},
   "source": [
    "## SPARK\n",
    "\n",
    "Auch SPARK hat eine Master/Slave-Architektur. Die Verteilung der Ressourcen haben wir auf dem Bild zur Topologie aufgeführt. Wir nutzen SPARK, da diese bezüglich Performance sehr gut optimiert ist (Logik von Actions und Transformations). Auch beinhaltet es die Möglichkeit, SQL-Queries auszuführen und übernimmt die \"Map-Reduce-Arbeiten\" in dem ein SparkJobs seine Stages (also Gruppen von Tasks) über die zur Verfügung stehenden Executors auszuführen und wieder zusammenführen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2e2a2-cd6c-4937-8860-e00d05b633df",
   "metadata": {},
   "source": [
    "## GitHub mit Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48122258-d9f7-47c5-8115-1eb960946f13",
   "metadata": {},
   "source": [
    "Das JupyterLab kann über GitHub (Link siehe unten) eingesehen werden. Die Dokumentation läuft über den User meberha und das GIT heisst \"BDLC-Projekt-G01\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b510f5-d564-409e-8058-1e98cc416d68",
   "metadata": {},
   "source": [
    "## Zugriff auf die Services\n",
    "\n",
    "Nachfolgend sind die URL's zu den einzelnen Services. Ist die VM wie auch die Services korrekt gestartet, können über diese Links auf die entsprechenden WebUI's zugegriffen werden:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f8dab-3aa5-4644-9f1a-3c8c5255bb70",
   "metadata": {},
   "source": [
    "| Service/Dienst | URL|\n",
    "| ------- | -------- |\n",
    "| HDFS DataNode | http://bdlc-05.el.eee.intern:9870/ |\n",
    "| SPARK Master | http://bdlc-05.el.eee.intern:8080/ |\n",
    "| SPARK History Server | http://bdlc-05.el.eee.intern:18080/ |\n",
    "| JupyterLab | http://bdlc-05.el.eee.intern:8888/ |\n",
    "| GitHub | https://github.com/meberha/BDLC-Project-G01 |\n",
    "| Dataset | https://www.kaggle.com/datasets/bulter22/airline-data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615154-1c62-4ec4-8414-7a0e939a223b",
   "metadata": {},
   "source": [
    "## Probleme im Cluster\n",
    "\n",
    "Das Aufsetzen des Cluster funktionierte (auch Dank der sehr guten Anleitung und den Dokumentationen im BDLC-Git) gut.\n",
    "\n",
    "Zu Beginn ist das Problem aufgetaucht, dass das ganze Dataset in das GitHub gepushed wurde, was duch die Grösse des Files nicht abgeschlossen werden konnte. Dies führte zu einem Fehler im GitHub von meberha. \n",
    "\n",
    "Da ein Mitglied seine VM für eine Berechnungsaufgabe in einem ML-Projekt verwendete, füllten die Files den Speicherplatz und beim Bearbeiten der Analyse konnten keine Jupyter-Änderungen mehr gespeichert werden. Sichtbar wurde dies mit dem Linux-Befehl \"df -h\" und zeigte bei /-Verzeichnis eine 100%-Usage. Durch das anschliessende Löschen der ML-Projekt-Dateien war das Verzeichnis nur noch zu 84% verwendet und noch 6.4GB frei.\n",
    "Anschliessend haben wir im /home Verzeichnis den Befehl \"du -h --max-depth=2\" ausgeführt und gesehen, dass im Verzeichnis \"cluster/.local\" mehrere GB benötigt werden. mit \"du -h --max-depth=2 cluster/.local/ \" war anschliessend ersichtlich, dass das Trash-Verzeichnis noch mehrere GB blockiert. Dies waren die gelöschten Datasets airline. Nach deren Entfernung  (mit rm filename.typ) zeigte auch unsere Master-VM wieder 20G freien Speicher im home-Verzeichnis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed677d0-9e9c-4b84-b226-bdd556139d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
